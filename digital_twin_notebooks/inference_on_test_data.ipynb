{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecff281b",
   "metadata": {},
   "source": [
    "# Inference on test data (2019-2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d116049f",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e64c0e8b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import toml\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from tqdm import tqdm\n",
    "from typing import Any\n",
    "from itertools import islice\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import matplotlib.markers as markers\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable    \n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.mpl.ticker import (LongitudeFormatter, LatitudeFormatter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f571c9",
   "metadata": {},
   "source": [
    "Add to system path the parent folder in order to be able to import the **Fires** library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a03acce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "_pth = \"../\"\n",
    "if _pth not in sys.path:\n",
    "\tsys.path.append(_pth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4785d525",
   "metadata": {},
   "source": [
    "Import from **Fires** library what is necessary to build the Inference workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65388acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/emanueledonno/VSCode/CMCC/ML4Fires/config\n",
      "/Users/emanueledonno/VSCode/CMCC/ML4Fires/digital_twin_notebooks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emanueledonno/opt/anaconda3/envs/cmcc-torch/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import Fires\n",
    "from Fires._datasets.torch_dataset import FireDataset\n",
    "\n",
    "from Fires._macros.macros import (\n",
    "    CONFIG,\n",
    "    DATA_DIR,\n",
    "    LOG_DIR,\n",
    "    NEW_DS_PATH,\n",
    "    RUN_DIR,\n",
    "    SCALER_DIR,\n",
    "    TORCH_CFG\n",
    ")\n",
    "\n",
    "from Fires._models.unetpp import UnetPlusPlus as UPP\n",
    "\n",
    "from Fires._scalers.minmax import MinMaxScaler\n",
    "from Fires._scalers.standard import StandardScaler\n",
    "\n",
    "from Fires._utilities.configuration import load_global_config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404149b0",
   "metadata": {},
   "source": [
    "## Experiments and plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dc24fc",
   "metadata": {},
   "source": [
    "### Plot functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc18e1c0",
   "metadata": {},
   "source": [
    "Define:\n",
    "- the projection type\n",
    "- the map extension\n",
    "- the arrays with the latitudes and the longitudes that must used for axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b5090e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define projection\n",
    "datacrs = ccrs.PlateCarree()\n",
    "\n",
    "# define map extent\n",
    "extent_args=dict(extents = [-180, 180, -90, 90], crs=datacrs)\n",
    "\n",
    "# define latitudes and longitudes array that must be used for axes\n",
    "latitudes = np.arange(-60, 90, 30)\n",
    "longitudes = np.arange(-160, 180, 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472cdd76",
   "metadata": {},
   "source": [
    "Function used to draw map features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42f40085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_features(ax:Any):\n",
    "\t\"\"\"\n",
    "\tThis function adds several geographical features to the map using Cartopy features:\n",
    "\n",
    "    * Political borders: outlines country borders with a solid black line style (':') and a linewidth of 0.5.\n",
    "    * Oceans: outlines the ocean regions with a solid black line style ('-') and a linewidth of 0.8.\n",
    "    * Lakes: outlines lakes with a solid black line style ('-') and a linewidth of 0.8.\n",
    "    * Rivers: outlines rivers with a solid black line style ('-') and a linewidth of 0.8.\n",
    "    * Coastlines: adds high-resolution coastlines (50 meters) to the map with a higher zorder (3) for better visibility.\n",
    "\n",
    "    **Note:** \n",
    "        * Land is not explicitly added in this function. \n",
    "        * Adding a background image using `ax.stock_img()` is not implemented. \n",
    "\n",
    "\tParameters\n",
    "\t----------\n",
    "\tax : Any\n",
    "\t\tThe matplotlib axis object to add the features to.\n",
    "\n",
    "\tReturns\n",
    "\t-------\n",
    "\tax : Any\n",
    "\t\tThe modified matplotlib axis object.\n",
    "\n",
    "\t\n",
    "    \n",
    "\t\"\"\"\n",
    "\t# political borders\n",
    "\tax.add_feature(cfeature.BORDERS, linestyle=':',linewidth=0.5, edgecolor='k')\n",
    "\t# add ocean\n",
    "\tax.add_feature(cfeature.OCEAN, linestyle='-',linewidth=0.8, edgecolor='k')\n",
    "\t# add lakes\n",
    "\tax.add_feature(cfeature.LAKES, linestyle='-',linewidth=0.8, edgecolor='k')\n",
    "\t# add rivers\n",
    "\tax.add_feature(cfeature.RIVERS, linestyle='-',linewidth=0.8, edgecolor='k')\n",
    "\t# add land\n",
    "\t# ax.add_feature(cfeature.LAND, zorder=1, edgecolor='k')\n",
    "\t# add coastlines\n",
    "\tax.coastlines(resolution='50m', zorder=3)\n",
    "\t\n",
    "\t# add stock image\n",
    "\t# ax.stock_img()\n",
    "\n",
    "\treturn ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16dae19",
   "metadata": {},
   "source": [
    "Function used to highlight specific burned areas points on map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b63bf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_ba(ax:Any, y:float, x:float, color:str):\n",
    "\t\"\"\"\n",
    "\tPlots lines and a circle to highlight a specific point on a map.\n",
    "\n",
    "\tParameters\n",
    "\t----------\n",
    "\tax : Any\n",
    "\t\tThe matplotlib axis object where to plot the elements.\n",
    "\ty : float\n",
    "\t\tThe latitude value of the point to highlight.\n",
    "\tx : float\n",
    "\t\tThe longitude value of the point to highlight.\n",
    "\tcolor : str\n",
    "\t\tThe color to use for the lines and circle.\n",
    "\n",
    "\tReturns\n",
    "\t-------\n",
    "\tax : Any \n",
    "\t\tThe modified matplotlib axis object.\n",
    "\t\"\"\"\n",
    "\t\n",
    "\t# plot lines corresponding to latitude and longitude value of burned areas\n",
    "\tax.axhline(y=y, color=color, linewidth=3, zorder=3, linestyle=':')\n",
    "\tax.text(-181.5, np.round(y, 2), f'{np.round(y, 2)}', color=color, fontweight='bold', size=50, ha='center', va='center', rotation=90)\n",
    "\t\n",
    "\tax.axvline(x=x, color=color, linewidth=3, zorder=3, linestyle=':')\n",
    "\tax.text(np.round(x, 2), -90.5, f'{np.round(x, 2)}', color=color, fontweight='bold', size=50, ha='center', va='top')\n",
    "\t\n",
    "\t# plot cicle around the pixel with the value of burned areas\n",
    "\tcircle = plt.Circle((x, y), 1, color=color, linewidth=5, fill=False, zorder=3)\n",
    "\tax.add_patch(circle)\n",
    "\t\n",
    "\treturn ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010c4974",
   "metadata": {},
   "source": [
    "Funtion to set the axis labels, ticks, and formatters for latitude and longitude:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e05ebc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_axis(ax, is_y:bool, latlon_vals, gl):\n",
    "\t\"\"\"\n",
    "\tSets the axis labels, ticks, and formatters for latitude or longitude on a map.\n",
    "\n",
    "\tParameters\n",
    "\t----------\n",
    "\tax : Any\n",
    "\t\tThe matplotlib axis object to modify.\n",
    "\tis_y : bool\n",
    "\t\tTrue if setting the y-axis, False for x-axis.\n",
    "\tlatlon_vals : np.array\n",
    "\t\tThe list of latitude or longitude values for the axis.\n",
    "\tgl : Any\n",
    "\t\tThe gridlines object for the map.\n",
    "\n",
    "\tReturns\n",
    "\t-------\n",
    "\tax : Any\n",
    "\t\tThe modified axis object.\n",
    "\t\"\"\"\n",
    "\tvalues = latlon_vals\n",
    "\tif is_y:\n",
    "\t\tlat_formatter = LatitudeFormatter()\n",
    "\t\tax.yaxis.set_major_formatter(lat_formatter)\n",
    "\t\tax.yaxis.set_major_locator(mticker.FixedLocator(values))\n",
    "\t\tax.set_yticklabels(values, fontweight='bold', size=50, rotation=90)\n",
    "\t\tax.set_yticks(values)\n",
    "\t\tgl.xlocator = mticker.FixedLocator(values)\n",
    "\t\tgl.xlines = False\n",
    "\n",
    "\telse:\n",
    "\t\tlon_formatter = LongitudeFormatter(zero_direction_label=True)\n",
    "\t\tax.xaxis.set_major_formatter(lon_formatter)\n",
    "\t\tax.xaxis.set_major_locator(mticker.FixedLocator(values))\n",
    "\t\tax.set_xticklabels(values, fontweight='bold', size=50)\n",
    "\t\tax.set_xticks(values)\n",
    "\t\tgl.ylocator = mticker.FixedLocator(values)\n",
    "\t\tgl.ylines = False\n",
    "\n",
    "\treturn ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994af775",
   "metadata": {},
   "source": [
    "Function to draw Tropic of Cancer, Equator, and Tropic of Capricorn on a map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcda5473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_tropics_and_equator(ax):\n",
    "\t\"\"\"\n",
    "\tPlots lines representing the Tropic of Cancer, Equator, and Tropic of Capricorn on a map.\n",
    "\n",
    "\tParameters\n",
    "\t----------\n",
    "\tax : Any\n",
    "\t\tThe matplotlib axis object where to plot the lines.\n",
    "\n",
    "\tReturns\n",
    "\t-------\n",
    "\tax : Any: \n",
    "\t\tThe modified matplotlib axis object.\n",
    "\t\"\"\"\n",
    "\tax.axhline(23.5, linestyle=':', color='blue', linewidth=0.7, label='Tropic of Cancer')\n",
    "\tax.axhline(0.00, linestyle=':', color='black', linewidth=0.7, label='Equator')\n",
    "\tax.axhline(-23.5, linestyle=':', color='blue', linewidth=0.7, label='Tropic of Capricorn')\n",
    "\treturn ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646a8858",
   "metadata": {},
   "source": [
    "Function that generates a comprehensive map visualization of the burned areas data, highlighting minimum and maximum values alongside their confidence intervals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e2bc805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dataset_map(\n",
    "\tavg_target_data:np.array,\n",
    "\tavg_data_on_lats:np.array,\n",
    "\tlowerbound_data:np.array,\n",
    "\tupperbound_data:np.array,\n",
    "\tlats:list,\n",
    "\tlons:list,\n",
    "\ttitle:str,\n",
    "\tcmap:str) -> None:\n",
    "\t\"\"\"\n",
    "\tGenerates a comprehensive map visualization of a dataset, highlighting \n",
    "\tminimum and maximum values alongside their confidence intervals.\n",
    "\n",
    "\tParameters\n",
    "\t----------\n",
    "\tavg_target_data : np.array\n",
    "\t\t2D array containing the core data to be visualized as color intensity on the map.\n",
    "\t\tMissing values (NaN) are handled by setting the color to transparent.\n",
    "\n",
    "\tavg_data_on_lats : np.array\n",
    "\t\t1D array containing the average of the target data for each latitude value.\n",
    "\t\tThis data is plotted as a line in a secondary subplot.\n",
    "\n",
    "\tlowerbound_data : np.array\n",
    "\t\t2D array containing the lower bound of the data (e.g., standard deviation or confidence interval) for each latitude and longitude.\n",
    "\t\tThis data is used to shade the area around the average line in the secondary subplot.\n",
    "\n",
    "\tupperbound_data : np.array\n",
    "\t\t2D array containing the upper bound of the data for each latitude and longitude.\n",
    "\t\tSimilar to `lowerbound_data`, it's used for shading the confidence interval in the secondary subplot.\n",
    "\n",
    "\tlats : list\n",
    "\t\tList containing the latitude values corresponding to the data.\n",
    "\n",
    "\tlons : list\n",
    "\t\tList containing the longitude values corresponding to the data.\n",
    "\n",
    "\ttitle : str\n",
    "\t\tThe title to be displayed at the top of the plot.\n",
    "\n",
    "\tcmap : str\n",
    "\t\tThe name of the colormap to use for visualizing the data on the map.\n",
    "\t\n",
    "\tReturns\n",
    "\t-------\n",
    "\tNone\n",
    "\t\tSaves the figure as a high-resolution PNG image (300 dpi) but does not return anything.\n",
    "\t\"\"\"\n",
    "\t# define color\n",
    "\tcolor = 'darkred' #fc6742 #4296fc #990e0e\n",
    "\t\n",
    "\t# compute maximum along latitudes and longitudes and find index\n",
    "\tmaximum_val = np.nanmax(avg_target_data)\n",
    "\tlat_idx_max, lon_idx_max  = np.where(avg_target_data==maximum_val)\n",
    "\tmax_val_latitude = lats[lat_idx_max][0]\n",
    "\tmax_val_longitude = lons[lon_idx_max][0]\n",
    "\t\n",
    "\t# compute minimum along latitudes and longitudes and find index\n",
    "\tminimum_val = np.nanmin(avg_target_data)\n",
    "\tlat_idx_min, lon_idx_min  = np.where(avg_target_data==minimum_val)\n",
    "\tmin_val_latitude = lats[lat_idx_min][0]\n",
    "\tmin_val_longitude = lons[lon_idx_min][0]\n",
    "\t\n",
    "\t# define fiure and subplots\n",
    "\t_, ax1 = plt.subplots(figsize=(90, 80), subplot_kw=dict(projection=datacrs), sharey=True)\n",
    "\t\n",
    "\t# set title of the plot\n",
    "\tax1.set_title(title, fontweight='bold', size=80)\n",
    "\t\n",
    "\t# set x and y labels\n",
    "\tax1.set_xlabel('Longitude [deg]', fontweight='bold', size=50)\n",
    "\tax1.set_ylabel('Latitude [deg]', fontweight='bold', size=50)\n",
    "\t\n",
    "\t# set map extent\n",
    "\tax1.set_extent(**extent_args)\n",
    "\t\n",
    "\t# plot map features such as borders, sea, lakes, rivers and background image\n",
    "\tax1 = draw_features(ax=ax1)\n",
    "\t\n",
    "\t# plot data on the map\n",
    "\tcmap = plt.get_cmap(cmap)\n",
    "\tcmap.set_under((0, 0, 0, 0))\n",
    "\th = ax1.pcolormesh(lons, lats, avg_target_data, transform=datacrs, cmap=cmap, zorder=3, alpha=0.5)\n",
    "\t\n",
    "\t# highlight pixel where the maximum value ov burned areas has been found and put a circle around it\n",
    "\tax1 = highlight_ba(ax=ax1, y=max_val_latitude, x=max_val_longitude, color=color)\n",
    "\t\n",
    "\t# highlight pixel where the minimum value ov burned areas has been found and put a circle around it\n",
    "\tax1 = highlight_ba(ax=ax1, y=min_val_latitude, x=min_val_longitude, color='green')\n",
    "\t\n",
    "\t# add grid lines for latitude and longitude\n",
    "\tgl = ax1.gridlines(crs=datacrs, draw_labels=False, linewidth=1.5, color='gray', alpha=0.5, linestyle='-', zorder=3)\n",
    "\t\n",
    "\t# define longitudes and set x ticks\n",
    "\tax1 = set_axis(ax=ax1, is_y=False, latlon_vals=longitudes, gl=gl)\n",
    "\t\n",
    "\t# define latitudes and set y ticks\n",
    "\tax1 = set_axis(ax=ax1, is_y=True, latlon_vals=latitudes, gl=gl)\n",
    "\t\n",
    "\t# define latitudes for tropics and equator\n",
    "\tax1 = draw_tropics_and_equator(ax=ax1)\n",
    "\t\n",
    "\t# add subplot\n",
    "\tdivider = make_axes_locatable(ax1)\n",
    "\tax2 = divider.append_axes(\"right\", size=\"10%\", pad=0.5, axes_class=plt.Axes)\n",
    "\t\n",
    "\t# plot data\n",
    "\tax2.plot(avg_data_on_lats, lats, color='red', linewidth=1)\n",
    "\tax2.plot(upperbound_data, lats, alpha=0.3, color='black', linewidth=0.5)\n",
    "\tax2.plot(lowerbound_data, lats, alpha=0.3, color='black', linewidth=0.5)\n",
    "\t\n",
    "\t# fill space between lines\n",
    "\tax2.fill_betweenx(y=lats, x1=avg_data_on_lats, x2=upperbound_data, color='gray', alpha=0.15)\n",
    "\tax2.fill_betweenx(y=lats, x1=avg_data_on_lats, x2=lowerbound_data, color='gray', alpha=0.15)\n",
    "\t\n",
    "\t# define latitudes for tropics (in degrees) and equator\n",
    "\tax2 = draw_tropics_and_equator(ax=ax2)\n",
    "\t\n",
    "\t# plot max position\n",
    "\tax2.axhline(max_val_latitude, color=color, linewidth=3)\n",
    "\t\n",
    "\t# plot min position\n",
    "\tax2.axhline(min_val_latitude, color='green', linewidth=3)\n",
    "\t\n",
    "\t# set x label\t\n",
    "\tax2.set_xlabel(' Mean ', fontweight='bold', size=50, labelpad=50)\n",
    "\t\n",
    "\t# create list of max values\n",
    "\tax2_vals = np.around([np.nanmin(lowerbound_data, axis=0), np.nanmax(avg_data_on_lats, axis=0), np.nanmax(upperbound_data, axis=0)], 2)\n",
    "\t\n",
    "\t# plot axes tick linesÂ§\n",
    "\tfor tick in ax2_vals:\n",
    "\t\tax2.axvline(x=tick, color='blue', alpha=1, linewidth=1, linestyle=':')\n",
    "\t\tax2.text(round(tick), -.005, f'{round(tick)}', color='blue', fontweight='bold', size=50, transform=ax2.get_xaxis_transform(), ha='center', va='top')\n",
    "\n",
    "\tax2.text(0, -.005, '0', color='black', fontweight='bold', size=50, transform=ax2.get_xaxis_transform(), ha='center', va='top') \n",
    "\t\n",
    "\tax2.set_xticks([])\n",
    "\tax2.set_yticks([])\n",
    "\tax2.set_ylim(bottom=-90, top=90)\n",
    "\tax2.margins(y=0)\n",
    "\t# ax2.autoscale_view(scaley=True)\n",
    "\t\t\n",
    "\t# add colorbar plot\n",
    "\tax_cb = divider.append_axes(\"right\", size=\"2%\", pad=0.3, axes_class=plt.Axes)\n",
    "\tcbar = plt.colorbar(h, ax_cb)\n",
    "\tcbar.ax.tick_params(labelsize=30)\n",
    "\tcbar.ax.set_ylabel('Hectares', color='black', fontweight='bold', size=50, labelpad=50, rotation=270)\n",
    "\t\n",
    "\tplt.tight_layout()\n",
    "\tplt.savefig(f\"./img/fcci {title}.png\", dpi=300)\n",
    "\t# plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315d7ece",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e57247",
   "metadata": {},
   "source": [
    "The first step in the inference pipeline is to select the neural network configuration (**experiment**) and the trained model with that configuration, to generate the burned area maps with the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed97bbc5-f63f-48ad-a260-015c8622a477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment path: /Users/emanueledonno/VSCode/CMCC/ML4Fires/experiments/20240311_222733\n"
     ]
    }
   ],
   "source": [
    "# define list of folders with the experiments\n",
    "experiment_paths = ['20240311_222733']\n",
    "\n",
    "# define main path to experiments folder\n",
    "EXP_PATH = os.path.join(os.path.dirname(os.getcwd()), \"experiments\")\n",
    "\n",
    "# define single experiment path\n",
    "single_path = os.path.join(EXP_PATH, str(experiment_paths[0]))\n",
    "print(f\"Experiment path: {single_path}\")\n",
    "\n",
    "# define a common dictionary with all the experiments\n",
    "exp_dicts = dict()\n",
    "for folder in experiment_paths:\n",
    "\tpath = os.path.join(EXP_PATH, folder)\n",
    "\tfor file in os.listdir(path):\n",
    "\t\tif file.endswith(\".toml\"):\n",
    "\t\t\texp_args = toml.load(os.path.join(path, file))\n",
    "\t\t\texp_dicts[file.split('.')[0]] = exp_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2945dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# __d = exp_dicts['exp_5']['model']\n",
    "# __p = __d['last_model']\n",
    "# __M_STATE = torch.load(__p, map_location=torch.device('cpu'))['model']\n",
    "\n",
    "# __args = {k: eval(v) if k == \"activation\" and v in globals() else v for k, v in sorted(__d['args'].items())}\n",
    "# mdl_cls = eval(__d['cls'])\n",
    "\n",
    "# model = mdl_cls(**__args)\n",
    "\n",
    "# model.load_state_dict(__M_STATE)\n",
    "\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "124fb02a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['exp_5'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_dicts.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50750267",
   "metadata": {},
   "source": [
    "This notebook showcases an example of using a preliminary version of the Digital Twin. In this first part, we will focus on creating the reference dataset for inference, which is a prerequisite to running the subsequent cells of the notebook. Steps:\n",
    "\n",
    "1. **Download the SeasFireCube v3 dataset**:\n",
    "\n",
    "\tDownload the [SeasFireCube v3](https://zenodo.org/records/8055879) dataset and save it in the `data` folder.\n",
    "\t\n",
    "1. **Load the dataset with `xarray`**\n",
    "\n",
    "\tImport the xarray library and load the SeasFireCube v3 dataset saved in the `data` folder.\n",
    "\n",
    "1. **Select variables**\n",
    "\n",
    "\tUse the experiment configuration `/experiments/20240311_222733/exp_5.toml` to identify the `drivers` and `targets` variables to be used in the model. Define these variables and select related data from previously loaded dataset.\n",
    "\n",
    "1. **Expand the `lsm` variable**\n",
    "\tThe `lsm` variable represents the land-sea mask. Expand this variable to cover the entire time range of the dataset.\n",
    "\n",
    "1. **Select the years of interest**\n",
    "\tSelect the years of interest for the analysis, in this case 2019 and 2020. Filter the dataset based on these years.\n",
    "\n",
    "1. **Save the reference dataset**\n",
    "\tSave the filtered and prepared reference dataset to the data folder.\n",
    "\n",
    "1. **Update the path in the configuration file**\n",
    "\tOpen the experiment configuration file `/experiments/20240311_222733/exp_5.toml`. Update the value of the `path_to_zarr` key with the full path to the saved reference dataset in the data folder.\n",
    "\n",
    "> [!NOTE]\n",
    "> The SeasFireCube v3 dataset is large in size and may not be possible to upload directly to GitHub.\n",
    "> Creating the reference dataset is a preliminary step required to run the subsequent cells of the notebook.\n",
    "> Make sure you have downloaded and saved the SeasFireCube v3 dataset correctly to the data folder before proceeding.\n",
    "> Verify that the path to the reference dataset is updated correctly in the experiment configuration file.\n",
    "\n",
    "\n",
    "After successfully completing the steps described in this section, you will have created the reference dataset necessary to run the subsequent cells of the notebook and test the Digital Twin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba2d739",
   "metadata": {},
   "source": [
    "The Python code in the cells below performs the Inference Phase in three steps:\n",
    "\n",
    "1. **Define some useful constants, maps and paths for the inference phase**\n",
    "\n",
    "\t* Create the folder $\\texttt{img}$ to store burned areas maps for prediction, real and difference data\n",
    "\t\t\t\t\n",
    "\t* Define the theoretichal maximum value of hectares that can be burned for a pixel\n",
    "\n",
    "\t* Load the $\\texttt{land-sea mask}$ and substitute zeros with NaN values\n",
    "\n",
    "\t* Define path to complete dataset in $\\texttt{zarr}$ format and load it\n",
    "\n",
    "\t* Define the test FireDataset and DataLoader class arguments as dictionaries\n",
    "\n",
    "\t* Load valid dates for test and define the test data\n",
    "\t\n",
    "\t* Load latitudes and longitude\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "981919c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder to store images\n",
    "os.makedirs(os.path.join(os.getcwd(), \"img\"), exist_ok=True)\n",
    "\n",
    "# define max hectares value \n",
    "MAX_HECTARES = pow((111/4), 2) * 100\n",
    "\n",
    "# load the land sea mask and substitute zeros with NaN values\n",
    "lsm = np.load(\"../data/landseamask.npy\")\n",
    "lsm[lsm == 0] = np.nan\n",
    "\n",
    "# define the land-sea map with max hectares values\n",
    "MAX_HECT_LSM_MAP = lsm * MAX_HECTARES\n",
    "\n",
    "# define path to complete dataset\n",
    "DS_PATH = os.path.join(os.path.dirname(os.getcwd()), \"data\", \"sfv03_fcci.zarr\")\n",
    "\n",
    "# test FireDataset and DataLoader args\n",
    "ds_args_ = dict(src=DS_PATH, years=list(range(2019, 2022)))\n",
    "dl_args_ = dict(batch_size=1,shuffle=True)\n",
    "\n",
    "# load valid dates for test and define the test data\n",
    "valid_dates = np.load(\"../data/valid_dates.npy\")\n",
    "main_data = xr.open_zarr(DS_PATH).sel(time = slice(str(valid_dates[0]), str(valid_dates[-1])))\n",
    "\n",
    "# load latitudes and longitudes\n",
    "lats = np.load(\"../data/latitudes.npy\")\n",
    "lons = np.load(\"../data/longitudes.npy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6953fc64",
   "metadata": {},
   "source": [
    "\n",
    "2. **Define functions to carry on the Inference Phase**\n",
    "\n",
    "\t* $\\texttt{get\\_model()}$ used to load the best trained Pytorch model and to set its parameters and weights\n",
    "\t\n",
    "\t* $\\texttt{get\\_scaler()}$ used to load mean and standard deviation maps in order to create a scaler object\n",
    "\t\n",
    "\t* $\\texttt{get\\_predictions()}$ used to make predictions on test data\n",
    "\t\n",
    "\t* $\\texttt{get\\_data\\_loader()}$ used to create the Pytorch Data Loader for test data\n",
    "\t\n",
    "\t* $\\texttt{compute\\_aggregated\\_data()}$ used to compute the mean or difference between data, and aggregate along latitudes and longitudes\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f135e782",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model(d:dict):\n",
    "\t\"\"\"\n",
    "\tCreates and loads a PyTorch model based on configuration parameters.\n",
    "\n",
    "\tParameters\n",
    "\t----------\n",
    "\td : dict\n",
    "\t\tA dictionary containing model configuration parameters. It must include the following keys:\n",
    "\n",
    "\t\t* args: A dictionary of keyword arguments to be passed to the model constructor.\n",
    "\t\t\t\tIf the key 'activation' is present, its value will be evaluated using `eval`.\n",
    "\t\t* cls: A string representing the fully qualified class name of the model (e.g., \"torch.nn.Linear\").\n",
    "\t\t* last_model: The path to the saved model state dictionary to be loaded.\n",
    "\n",
    "\tReturns\n",
    "\t-------\n",
    "\ttorch.nn.Module\n",
    "\t\tThe instantiated and loaded PyTorch model.\n",
    "\n",
    "\tNotes\n",
    "\t-----\n",
    "\tThis function uses the `eval` function to dynamically evaluate the 'activation' argument\n",
    "\tif it is present in the configuration dictionary. This allows for flexibility in defining\n",
    "\tactivation functions.\n",
    "\n",
    "\tThe model's state dictionary is loaded from the file specified by `last_model`. This\n",
    "\tassumes that the file was created by saving the model using `torch.save`.\n",
    "\t\"\"\"\n",
    "\n",
    "\t# create a dictionary with all model arguments\n",
    "\targs_ = dict()\n",
    "\t# define model arguments\n",
    "\tfor k in d['args'].keys():\n",
    "\t\targs_[k] = eval(d['args'][k]) if k == \"activation\" else d['args'][k]\n",
    "\n",
    "\t# get model class\n",
    "\tmdl_cls = eval(d['cls'])\n",
    "\t# define mdel\n",
    "\tmodel = mdl_cls(**args_)\n",
    "\t# get model path\n",
    "\tpath = d['last_model']\n",
    "\t# load model from path\n",
    "\tlm_state = torch.load(path, map_location=torch.device('cpu'))['model']\n",
    "\t# load weights\n",
    "\tmodel.load_state_dict(lm_state)\n",
    "\t# evaluate model\n",
    "\tmodel.eval()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "def get_scaler(d:dict, features:list):\n",
    "\t\"\"\"\n",
    "\tLoads mean and standard deviation datasets and creates a scaler for features.\n",
    "\n",
    "\tParameters\n",
    "\t----------\n",
    "\td : dict\n",
    "\t\tA dictionary containing configuration information. It must have a 'paths' key\n",
    "\t\twith the following sub-keys:\n",
    "\n",
    "\t\t\t* **fcci_mean_point_map:** The path to the dataset containing the mean values.\n",
    "\t\t\t* **fcci_stdv_point_map:** The path to the dataset containing the standard deviation values.\n",
    "\t\t\t* **cls:**  The scaler class to use (provided as a string for dynamic evaluation).\n",
    "\n",
    "\tfeatures : list[str]\n",
    "\t\tA list of strings representing the names of features to be scaled.\n",
    "\n",
    "\tReturns\n",
    "\t-------\n",
    "\tscaler_cls\n",
    "\t\tAn instance of the specified scaler class, initialized with the loaded\n",
    "\t\tmean and standard deviation datasets and the given features.\n",
    "\n",
    "\tRaises\n",
    "\t------\n",
    "\tKeyError\n",
    "\t\tIf any of the required keys ('paths', 'fcci_mean_point_map', 'fcci_stdv_point_map', or 'cls')\n",
    "\t\tare missing in the input dictionary.\n",
    "\t\"\"\"\n",
    "\n",
    "\t# load mean and standard deviation point dataset in order to scale test data\n",
    "\ttry:\n",
    "\t\tmean_ds = xr.load_dataset(d['paths']['fcci_mean_point_map']).load()\n",
    "\t\tstdv_ds = xr.load_dataset(d['paths']['fcci_stdv_point_map']).load()\n",
    "\texcept KeyError as e:\n",
    "\t\traise KeyError(f\"Missing required key in input dictionary: {e}\")\n",
    "\t\n",
    "\t# define scaler for drivers\n",
    "\tscaler_cls = eval(d['cls'].split(\"'\")[1])\n",
    "\tx_scaler = scaler_cls(mean_ds=mean_ds, stdv_ds=stdv_ds, features=features)\n",
    "\treturn x_scaler\n",
    "\n",
    "\n",
    "def get_predictions(model, data_loader) -> np.ndarray:\n",
    "\t\"\"\"\n",
    "\tGenerates predictions from a model using a DataLoader.\n",
    "\n",
    "\tThis function takes a trained PyTorch model and a DataLoader containing\n",
    "\tinput data. It iterates over the DataLoader, generates predictions for each\n",
    "\tbatch, and returns the predictions stacked vertically as a NumPy array.\n",
    "\n",
    "\tParameters\n",
    "\t----------\n",
    "\tmodel : torch.nn.Module\n",
    "\t\tThe trained PyTorch model to use for predictions.\n",
    "\tdata_loader : torch.utils.data.DataLoader\n",
    "\t\tThe DataLoader containing the input data to be used for predictions.\n",
    "\n",
    "\tReturns\n",
    "\t-------\n",
    "\tnp.ndarray\n",
    "\t\tA NumPy array containing the model's predictions. Each row represents\n",
    "\t\ta single prediction.\n",
    "\t\t\n",
    "\tNotes\n",
    "\t-----\n",
    "\tThe maximum number of predictions to generate is currently hardcoded to 92.\n",
    "\tThis can be modified if needed.\n",
    "\n",
    "\tThis function assumes that the model's forward method takes a batch of input\n",
    "\tdata and returns a batch of predictions.\n",
    "\t\"\"\"\n",
    "\t# define predictions list, predict data and store them\n",
    "\tpreds = []\n",
    "\twith torch.no_grad():\n",
    "\t\tfor i, (data, _) in enumerate(tqdm(data_loader)):\n",
    "\t\t\tif i >= 92: break\n",
    "\t\t\tpreds.append(model(data))\n",
    "\t\tprint(len(preds))\n",
    "\n",
    "\t# define predictions array as vertical stack of predictions list\n",
    "\treturn np.vstack(preds)\n",
    "\n",
    "\n",
    "def get_data_loader(scaler, drivers:list, targets:list, d:dict) -> torch.utils.data.DataLoader:\n",
    "\t\"\"\"\n",
    "\tCreates a PyTorch DataLoader for test data using a FireDataset.\n",
    "\n",
    "\tParameters\n",
    "\t----------\n",
    "\tscaler : object\n",
    "\t\tThe scaler object used to normalize driver features.\n",
    "\tdrivers : list[str]\n",
    "\t\tA list of strings representing the names of driver features.\n",
    "\ttargets : list[str]\n",
    "\t\tA list of strings representing the names of target features.\n",
    "\td : dict\n",
    "\t\tA dictionary containing additional configuration parameters such as:\n",
    "\t\t\t* **drop_reminder** (*bool*): Whether to drop the last incomplete batch if it doesn't match the batch size.\n",
    "\t\t\t* Additional parameters from ``data_params`` are expected (e.g., ``batch_size``, ``shuffle``).\n",
    "\n",
    "\tReturns\n",
    "\t-------\n",
    "\ttorch.utils.data.DataLoader\n",
    "\t\tA PyTorch DataLoader that provides batches of scaled and prepared data \n",
    "\t\tfor testing the fire prediction model.\n",
    "\t\"\"\"\n",
    "\n",
    "\t# define FireDataset arguments and FireDataset torch dataset for test data\t\n",
    "\ttest_torch_ds = FireDataset(\n",
    "\t\t**ds_args_,\n",
    "\t\tdrivers=drivers,\n",
    "\t\ttargets=targets,\n",
    "\t\tscalers=[scaler, None]\n",
    "\t)\n",
    "\t\n",
    "\t# define test data loader\n",
    "\ttest_loader = torch.utils.data.DataLoader(\n",
    "\t\ttest_torch_ds,\n",
    "\t\t**dl_args_,\n",
    "\t\tdrop_last=d['drop_reminder']\n",
    "\t)\n",
    "\n",
    "\treturn test_loader\n",
    "\n",
    "\n",
    "def compute_aggregated_data(data, other_data=None, operation=\"mean\") -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "\t\"\"\" Compute the mean or difference between data, and aggregate along latitudes and longitudes\n",
    "\n",
    "\tArgs:\n",
    "\t\tdata : numpy.ndarray\n",
    "\t\t \tInput data, can be unscaled or already scaled and masked depending on the operation to be performed\n",
    "\t\tother_data : numpy.ndarray\n",
    "\t\t \tOptional input data for calculating the difference, also assumed to be scaled and masked\n",
    "\t\toperation : str\n",
    "\t\t\tOperation to perform (\"mean\" for mean, \"diff\" for difference)\n",
    "\n",
    "\tReturns:\n",
    "\t\tA tuple containing:\n",
    "\t\t\t- Scaled and masked data\n",
    "\t\t\t- Mean along latitudes\n",
    "\t\t\t- Mean along longitudes\n",
    "\t\"\"\"\n",
    "\n",
    "\tif operation == \"diff\":\n",
    "\t\tif other_data is None:\n",
    "\t\t\traise ValueError(\"other_data must be provided when operation is 'diff'\")\n",
    "\t\tdata -= other_data  # difference between data that has been masked and rescaled to the original size\n",
    "\telse:\n",
    "\t\tdata *= MAX_HECT_LSM_MAP # mask data with the land sea mask and rescale to original size\n",
    "\t\n",
    "\tdescaled_on_lats = np.nanmean(data, axis=1)\n",
    "\tdescaled_on_lons = np.nanmean(data, axis=0)\n",
    "\n",
    "\tprint(f\" {operation.capitalize()} of data: {data.shape}\")\n",
    "\tprint(f\" Max: {np.nanmax(data)}\")\n",
    "\tprint(f\" Min: {np.nanmin(data)}\")\n",
    "\tprint(f\" Lats: {np.nanmax(descaled_on_lats)}\")\n",
    "\tprint(f\" Lons: {np.nanmax(descaled_on_lons)}\\n\")\n",
    "\n",
    "\treturn data, descaled_on_lats, descaled_on_lons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c9cf62",
   "metadata": {},
   "source": [
    "\n",
    "3. **Iterate throught experiments and make predictions**\n",
    "\n",
    "\tIn this section there is a for-loop that iterates through experiments in a dictionary named `exp_dicts` and performs the following tasks for each experiment:\n",
    "\n",
    "\t* Loads the experiment configuration.\n",
    "\n",
    "\t* Loads a pre-trained model.\n",
    "\n",
    "\t* Defines a scaler to normalize the test data.\n",
    "\n",
    "\t* Creates a torch dataset and data loader for the test data.\n",
    "\n",
    "\t* Predicts burned areas using the loaded model.\n",
    "\n",
    "\t* Loads real burned area data.\n",
    "\n",
    "\t* Calculates statistics (mean, standard deviation) for both predicted and real burned areas.\n",
    "\n",
    "\t* Calculates the difference between predicted and real burned areas.\n",
    "\t\n",
    "\t* Plots the result maps for predicted, real, and difference data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5440f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for key in exp_dicts.keys():\n",
    "\t\n",
    "\t# define current experiment\n",
    "\tcurr_exp = exp_dicts[key]\n",
    "\texp_name = curr_exp['exp_name']\n",
    "\t\n",
    "\t# Define model\n",
    "\tmodel = get_model(d=curr_exp['model'])\n",
    "\n",
    "\t# Create dataset for testing the model\n",
    "\n",
    "\t# get drivers and targets\n",
    "\tdrivers, targets = curr_exp['features']['drivers'], curr_exp['features']['targets']\n",
    "\t# define test datasets for drivers and targets\n",
    "\tdriver_ds, target_ds = main_data[drivers].load(), main_data[targets].load()\n",
    "\t# Scaler\n",
    "\tx_scaler = get_scaler(d=curr_exp['scalers'], features=drivers)\n",
    "\t# Create test data loader\n",
    "\ttest_loader = get_data_loader(scaler=x_scaler, drivers=drivers, targets=targets, d=curr_exp['trainer'])\n",
    "\t# Predict burned areas\t\n",
    "\tpreds_arr = get_predictions(model=model, data_loader=test_loader)\t\n",
    "\t\n",
    "\t# ------------------------------------------------------------------------------------\n",
    "\t\n",
    "\t# real target data (temporal mean)\n",
    "\tavg_real_on_time = target_ds.fcci_ba.mean(dim='time', skipna=True).data\n",
    "\tavg_real_descaled, avg_real_on_lats, avg_real_on_lons = compute_aggregated_data(data=avg_real_on_time)\n",
    "\n",
    "\t# real target data (temporal standard deviation)\n",
    "\tstd_real_on_time = target_ds.fcci_ba.std(dim='time', skipna=True).data\n",
    "\tstd_real_descaled, std_real_on_lats, std_real_on_lons = compute_aggregated_data(data=std_real_on_time)\n",
    "\t\n",
    "\t# define upperbound and lowerbound data for plotting the average on latitudes\n",
    "\treal_upperbound = avg_real_on_lats + std_real_on_lats\n",
    "\treal_lowerbound = avg_real_on_lats - std_real_on_lats\n",
    "\n",
    "\tplot_dataset_map(\n",
    "\t\tavg_target_data=avg_real_descaled,\n",
    "\t\tavg_data_on_lats=avg_real_on_lats,\n",
    "\t\tlowerbound_data=real_lowerbound,\n",
    "\t\tupperbound_data=real_upperbound,\n",
    "\t\tlats=lats,\n",
    "\t\tlons=lons,\n",
    "\t\ttitle = f'FCCI Burned Areas - {exp_name.upper()} Real',\n",
    "\t\tcmap = 'nipy_spectral_r') # 'CMRmap'\n",
    "\t\n",
    "\t# ------------------------------------------------------------------------------------\n",
    "\t\n",
    "\t# compute temporal mean for prediciton\n",
    "\tavg_preds_on_time = np.nanmean(preds_arr, axis=0)[0, ...]\n",
    "\tavg_preds_descaled, avg_preds_on_lats, avg_preds_on_lons = compute_aggregated_data(data=avg_preds_on_time)\n",
    "\t\n",
    "\t# compute temporal standard deviation for prediction\n",
    "\tstd_preds_on_time = np.nanstd(preds_arr, axis=0)[0, ...]\n",
    "\tstd_preds_descaled, std_preds_on_lats, std_preds_on_lons = compute_aggregated_data(data=std_preds_on_time)\n",
    "\n",
    "\t# define upperbound and lowerbound data for plotting the average on latitudes\n",
    "\tpreds_upperbound = avg_preds_on_lats + std_preds_on_lats\n",
    "\tpreds_lowerbound = avg_preds_on_lats - std_preds_on_lats\n",
    "\n",
    "\tplot_dataset_map(\n",
    "\t\tavg_target_data=avg_preds_descaled,\n",
    "\t\tavg_data_on_lats=avg_preds_on_lats,\n",
    "\t\tlowerbound_data=preds_lowerbound,\n",
    "\t\tupperbound_data=preds_upperbound,\n",
    "\t\tlats=lats,\n",
    "\t\tlons=lons,\n",
    "\t\ttitle = f'Predicted Burned Areas', # FCCI Burned Areas - {exp_name.upper()} Predictions',\n",
    "\t\tcmap = 'nipy_spectral_r') # 'CMRmap'\n",
    "\t\n",
    "\t# ------------------------------------------------------------------------------------\n",
    "\t\n",
    "\t# compute the difference between real and predicted data\n",
    "\tprint(f\" Mean difference\")\n",
    "\tavg_difference, avg_diff_on_lats, avg_diff_on_lons = compute_aggregated_data(data=avg_real_descaled, other_data=avg_preds_descaled, operation='diff')\n",
    "\t\n",
    "\tprint(f\" Stdv difference\")\n",
    "\tstd_difference, std_diff_on_lats, std_diff_on_lons = compute_aggregated_data(data=std_real_descaled, other_data=std_preds_descaled, operation='diff')\n",
    "\t\n",
    "\t# define upperbound and lowerbound data for plotting the average on latitudes\n",
    "\tdiff_upperbound = avg_diff_on_lats + std_diff_on_lats\n",
    "\tdiff_lowerbound = avg_diff_on_lats - std_diff_on_lats\n",
    "\tprint(f\"Upperbound: {np.nanmin(diff_upperbound)} \\t {np.nanmax(diff_upperbound)}\\n\")\n",
    "\tprint(f\"Lowerbound: {np.nanmin(diff_lowerbound)} \\t {np.nanmax(diff_lowerbound)}\\n\")\n",
    "\t\n",
    "\tprint(np.where(avg_difference==np.nanmax(avg_difference)))\n",
    "\t__x_idx_max, __y_idx_max= np.where(avg_difference==np.nanmax(avg_difference))\n",
    "\tprint(lats[__x_idx_max], lons[__y_idx_max])\n",
    "\t\n",
    "\tplot_dataset_map(\n",
    "\t\tavg_target_data=avg_difference,\n",
    "\t\tavg_data_on_lats=avg_diff_on_lats,\n",
    "\t\tlowerbound_data=diff_lowerbound,\n",
    "\t\tupperbound_data=diff_upperbound,\n",
    "\t\tlats=lats,\n",
    "\t\tlons=lons,\n",
    "\t\ttitle = f'FCCI Burned Areas - {exp_name.upper()} Difference',\n",
    "\t\tcmap = 'gist_rainbow_r') # 'CMRmap'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb53ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmcc-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
